---
name: galiana
description: Show discipline analytics — defect escape rate, override rate, cost metrics, and agent scorecard
---

# Galiana — Discipline Analytics

Use this skill to render Galiana KPI output from the local cache in a consistent diagnostic-first view.

## Step 1: Run analyzer

Locate the analyzer in plugin cache first, then in local dev checkouts:

```bash
GALIANA_SCRIPT=$(find ~/.claude/plugins/cache -path '*/clavain/*/galiana/analyze.py' 2>/dev/null | head -1)
[[ -z "$GALIANA_SCRIPT" ]] && GALIANA_SCRIPT=$(find ~/projects -path '*/os/clavain/galiana/analyze.py' 2>/dev/null | head -1)
```

If found, run:

```bash
CLAVAIN_PLUGIN_DIR="$(dirname "$(dirname "$GALIANA_SCRIPT")")"
python3 "$CLAVAIN_PLUGIN_DIR/galiana/analyze.py"
```

If no script is found, explain that Galiana is not installed and stop.

## Step 2: Read KPI cache

Read `~/.clavain/galiana-kpis.json`. If it does not exist or has no KPI payload, show a getting-started message that explains KPI events are generated by disciplined review/testing workflows and defect logging.

## Step 3: Present results

Render the response in this format:

```text
Galiana — Discipline Analytics
════════════════════════════════
Period: YYYY-MM-DD to YYYY-MM-DD
Beads shipped: N | Events analyzed: N

KPIs (diagnostic-first)
────────────────────────
[worst metric first]
[...]
[healthiest metric last]

Agent Scorecard
────────────────
Agent             Findings  P0  P1
fd-architecture   5         0   2
fd-quality        3         1   1

Advisories
──────────
- [info] ...
- [warn] ...
```

## KPI rules

- Order KPIs diagnostically: worst metrics first, healthiest last.
- Format every KPI exactly as: `KPI Name: value (numerator/denominator)`.
- Include the Agent Scorecard table when scorecard data exists.
- List advisories at the end, preserving severity labels like `[info]`, `[warn]`, `[critical]`.

## Topology Experiment Results

If `topology_efficiency.available` is true in the KPI data:

Show a recall heatmap table:

```text
Topology Efficiency (recall vs production)
──────────────────────────────────────────
Task Type      T2    T4    T6    T8    Samples
code_review    0.60  0.85  0.95  0.95  12
planning       0.70  0.90  0.92  0.92  8
docs           0.55  0.80  0.90  0.92  10
refactor       0.65  0.88  0.93  0.94  6
bugfix         0.75  0.92  0.95  0.95  4
```

Highlight the "sweet spot" — the smallest topology that achieves >=90% recall.

If fewer than 20 total experiments, add note: "Data still accumulating — run more experiments for reliable patterns."

## Eval Harness Health

If `eval_health.available` is true in the KPI data:

Show a fixture-level results table:

```text
Eval Harness Health
──────────────────────────────────────────
Overall: 85% property pass rate | Avg recall: 0.92 | 20 total runs

Fixture                  Pass Rate  Recall  Runs
synth-sql-injection      1.00       0.95    4
synth-n-plus-one         0.75       0.85    4
synth-tight-coupling     1.00       0.90    4
synth-contradictory-prd  1.00       0.88    4
synth-stale-docs         0.80       0.92    4
```

If any fixture has pass_rate < 1.0, highlight it as needing attention.

If fewer than 10 total runs, add note: "Limited data — run more eval cycles for reliable trends."

## Offer per-bead drill-down

After the report, offer:

`Want per-bead analytics? Run Galiana with --bead <id>.`
