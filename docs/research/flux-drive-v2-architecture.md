# Flux-Drive v2: Agent Architecture Redesign

## Problem Statement

The current flux-drive architecture has several pain points:

1. **Bloated roster** — 19 plugin agents, many rarely selected, high maintenance burden
2. **Wrong granularity** — 5 separate language reviewers (Go, Python, TypeScript, Shell, Rust) while other agents are too broad
3. **Static roster doesn't scale** — adding new review domains means new agent files, triage scoring is fragile
4. **Project agents are dead** — optional `fd-*.md` project agents rarely get created by users
5. **No learning** — each review starts from zero, findings don't compound across runs

## Decided Architecture: Adaptive Core + Dynamic Generation + Compounding

### Agent Roster (5 Core + Oracle + Ad-hoc)

Replace 19 specialized agents with 5 core agents that cover the same domains through merging:

| Agent | Merges | Focus |
|-------|--------|-------|
| **Architecture & Design** | architecture-strategist, pattern-recognition, code-simplicity | Boundaries, patterns, coupling, unnecessary complexity |
| **Safety & Correctness** | security-sentinel, data-integrity, concurrency, deployment-verification | Threats, data safety, race conditions, deploy risk |
| **Quality & Style** | fd-code-quality, all 5 language reviewers | Naming, conventions, test approach — auto-detects language from context |
| **User & Product** | fd-user-experience, product-skeptic, user-advocate, spec-flow-analyzer | User flows, value prop, UX friction, missing flows |
| **Performance** | performance-oracle | Bottlenecks, resource usage, scaling concerns |

**Oracle** (6th, cross-AI): GPT-5.2 Pro perspective via Oracle CLI. Gets diversity bonus in triage.

**Ad-hoc Agents**: Generated by triage when it detects a domain none of the 5 core agents cover well (e.g., GraphQL schema design, accessibility, i18n). Saved to `.claude/flux-drive/agents/` in the project repo. Reused if triage matches them in future runs. Graduate to Clavain-global if used across multiple projects.

**Removed**: Static project agents (`fd-*.md` bootstrapped by Codex) — replaced by ad-hoc generation.

### Triage Changes

Current triage scores each of 19 agents on a 0-2 scale with bonuses, caps at 8. New triage:

- Scores 5 core agents (most runs select 3-5 of them)
- Checks saved ad-hoc agents for domain match
- If unmatched domain detected, generates new ad-hoc agent prompt on the fly
- Oracle always offered if available
- Cap drops from 8 to 6 (fewer, smarter agents)

### Knowledge Layer (Two-Tier with qmd)

#### Project-Local Knowledge
**Location:** `.claude/flux-drive/knowledge/` in the project repo

Codebase-specific patterns discovered during reviews:
- Known trouble spots in the codebase
- Recurring issues specific to this project's architecture
- Saved ad-hoc agents generated for this project

#### Clavain-Global Knowledge
**Location:** `config/flux-drive/knowledge/` in the Clavain repo

Universal patterns that apply across all projects:
- Systematic blind spots ("Oracle consistently catches X that Claude misses")
- Cross-project heuristics ("convergence=1 from Performance agent is correct 80% for N+1 queries")
- Graduated ad-hoc agents proven across 2+ projects

#### Knowledge Format

Structured markdown with YAML frontmatter:
```yaml
---
domain: safety
source: flux-drive/2026-02-10
confidence: high
convergence: 3
origin: cross-ai-delta
lastConfirmed: 2026-02-10
---
Auth middleware in middleware/auth.go swallows context cancellation errors.
Both Safety agent and Oracle flagged this independently.
```

#### Retrieval via qmd

Agents don't use domain-tag matching. Instead, qmd semantic search retrieves relevant knowledge entries based on the document being reviewed and the agent's focus area. Frontmatter tags serve as fallback filters.

Freshness signal: `lastConfirmed` date updated when a finding is re-observed. Entries not confirmed in N reviews get moved to `archived/` subdirectory.

### Compounding System (Immediate + Async Deep-Pass)

#### Immediate Compounding Agent (runs after each synthesis)

A dedicated agent that runs after Phase 3 synthesis completes:

1. Reads the synthesis summary + cross-AI delta (not raw agent outputs)
2. Decides what's worth remembering permanently vs. what's review-specific
3. Extracts findings into knowledge entries (both project-local and global)
4. Updates `lastConfirmed` on re-observed findings
5. Checks if any ad-hoc agents should graduate to global

**Key judgment:** A high-confidence finding might be important for *this* review but too obvious to compound. A low-convergence Oracle finding might reveal a systematic Claude blind spot worth compounding. The compounding agent specializes in this distinction.

#### Async Deep-Pass Agent (runs periodically across reviews)

Scans `docs/research/flux-drive/` output directories across multiple reviews:

1. Identifies cross-review patterns that individual runs missed
2. Consolidates similar findings into higher-level patterns
3. Detects systematic agent blind spots across runs
4. Performs decay: archives findings not re-confirmed across recent reviews
5. Compounds its own discoveries back into the knowledge layer

Triggered manually or on a schedule (e.g., after every 5th flux-drive run).

### Phase Structure (Updated)

| Phase | What | Changes from v1 |
|-------|------|-----------------|
| **1. Triage** | Profile document, score core agents, check ad-hoc roster, optionally generate new ad-hoc agent | Simpler scoring (5 vs 19), ad-hoc generation |
| **2. Launch** | Inject knowledge context, dispatch agents in parallel | Each agent gets relevant knowledge entries prepended |
| **3. Synthesize** | Validate, deduplicate, convergence tracking, summary | Unchanged |
| **4. Cross-AI** | Oracle delta analysis | Unchanged |
| **5. Compound** | Extract learnings, write to knowledge layers | NEW PHASE |

### Open Questions

1. **Knowledge injection token budget**: How many knowledge entries should each agent receive? Too many = noise, too few = missing context. Possible: cap at 10 entries per agent, ranked by qmd relevance score.

2. **Ad-hoc agent graduation criteria**: What triggers promotion from project-local to Clavain-global? "Used in 2+ projects" is simple but might be too aggressive. Could require "used in 2+ projects AND produced high-confidence findings."

3. **Memory storage choice**: Three options under consideration:
   - Option 1: Structured markdown files with qmd semantic search (zero new infrastructure)
   - Option 2: SQLite + vector embeddings (more powerful retrieval, new dependency)
   - Option 3: qmd as the memory engine — markdown files, qmd handles retrieval, compounding agents handle consolidation/decay by rewriting files (recommended, zero new infrastructure)

   Current lean: Option 3. qmd is already in the stack, files stay human-readable and git-diffable, no new infrastructure.

4. **Deep-pass trigger**: Manual? After N runs? Calendar-based? Probably manual initially, with a counter in knowledge metadata tracking reviews-since-last-deep-pass.

5. **Merged agent quality**: Will 5 merged agents actually perform as well as 19 specialists? Risk: a single "Safety & Correctness" agent covering security + data integrity + concurrency + deployment might produce shallower findings than 4 dedicated agents. Mitigation: knowledge injection means the merged agent has richer context than any individual specialist did.
